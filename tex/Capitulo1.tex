\chapter{Introduction}\label{ch_introduction}%

\section{Problem Definition}

Among the different causes that lead a system to fail, some can be tracked back to design decisions in early system development process. The systematization provided by requirements engineering has been used to improve the quality of the delivered system documents, models and specification. Nonetheless, in many cases the RE process do not further investigate or verify how the system-to-be will perform based on its design model, relying only on later execution testing.

% both strategically through higher level goals and operationally through lower level goals and tasks

Non-functional dependability requirements are important metrics related to the correct system operation. Dependability defines the ability of delivering a service that can justifiably be trusted. Fail forecasting should provide a qualitative and/or a quantitative evaluation of the system behaviour in respect to fault occurrence or activation, while fault removal includes the verification, diagnosis and correction~[AVIZIENIS]. For some systems, the late fault correction may be very expensive and, in the worse case, may happen after a catastrophic event~[DEPENDABILITY].

In traditional Goal-oriented requirements engineering (GORE) methodologies [GORE CAC], contribution  analysis is based on domain knowledge about the positive, neutral (implicit) or negative impact of a given system alternative to one or more system goals, generally a qualitative goal. By  comparing the overall contribution of two or more alternatives, a decision is made about which one should be adopted for the system-to-be. For instance, if one goal is to communicate with a remote user mobile, alternative means for the notification agent could be to send a SMS, an internet based message or a voice call. These alternatives may contribute with different values for qualitative goals such as `reliable delivery', `fast delivery', `convenient delivery', etc. 

The problem with this approach is threefold. First, it is based on domain knowledge information that may not exist or may not be precise and reliable. As a consequence, the decision of which alternative to use, either at design time or at runtime, may be biased and lead to unexpected violations - the selected alternative may be unable to fulfil its qualitative goals. Second, it is limited to a static representation without any dynamic information that could be used to verify quality attributes that depend on system behaviour and its many nuances such as execution order, cardinality and execution priority. Finally, contribution links are deterministic. As such, the design decision based on contribution analysis is reduced to a simple sum comparison of the concurring alternatives contributions with no support for probabilistic verification.

Additionally to the contribution analysis limitation, the context in which systems operate may not be static. Mobile and pervasive computing, among others, are examples of new computer paradigms for which the environment is dynamic. Battery, signals strength, components availability and the quality of physical resources and relevant information such as the user geographic location may vary through time, posing a new sort of challenge to the development of socio-technical systems based on these paradigms. The contextualization of the informations gathered at RE phase becomes imperative once its validity may be threatened by changing environment conditions~[Finkelstein, CGM]. Accordingly, any improvement of the GORE contribution analysis may have to consider multiple operational contexts.

Static model-based verification is a powerful approach to evaluate metrics over a finite-state model representing the system behaviour. The advantage of this approach is to reduce the correction costs by anticipating failures at early stages of the development cycle. Increased analysis overhead is one of the drawbacks of this approach. Thus, its application must be well justified by the criticality level of the system and by the adoption of an appropriated model-based verification approach.

Despite its valuable contribution to the requirements analysis and engineering, GORE still lacks the proper means to verify the conformance of some important metrics such as dependability requirements and others non-functional requirements (NFR) related to the likelihood and frequency of system execution failures. By tackling the verification of these metrics with a more precise and reliable approach, this work aims to mitigate the occurrence of failures at an early phase of the system development that must justifiably be trusted.

%for these systems must also address the problem of multiple contexts of operation.

%The contextualization of an information means that its validity is not absolute in respect to the state of the world it relates to.

\section{Proposed Solution}

In order to provide a more solid and precise approach for the non-functional verification of different system alternatives and to improve the GORE contribution analysis, we propose the extension of the TROPOS goal-oriented software development methodology with a probabilistic model checking (PMC) approach that has already been explored and is supported by tools such as PRISM model checker[gena√≠na PMC, PRISM]. PMC is a formal method for static verification of system models. System models should represent specified activities that fulfils the root goal or any lower level goal and its elicited alternatives. This model should then be checked for properties such as reliability, availability, performance and power consumption.

The PMC technique requires a behaviour system specification. As the goal model proposed in TROPOS is static, no information regarding achievement/execution order, cardinality and priority of goals/tasks is available, except the activity diagram for the detailing of an agent's single capability behaviour and the sequence diagram for agents interaction. Nonetheless, this problem was tackled by Dalpiaz et al. with a regular expression language to express runtime information, e.g., how many times the same goal should be achieved and the execution order of different system tasks~[RGM]. 

We have used the RGM proposal to fill the gap between the static goal model and its dynamic representation. This dynamic view of the goal model is translated to a probabilistic model following the PMC technique and the PRISM model checker as the tool to automate the verification. PRISM language is used to define the probabilistic model and the Probabilistic Computation Tree Logic (PCTL) to describe the properties to be verified in the model. These properties are derived from the NFR associated to corresponding system goals being verified.

To address the problem of a dynamic context of operation, the context effects over goals, means and metrics should be parametrized to produce a formula that can check the system and its alternatives for different contexts. This verification, performed as part of the Validation \& Verification (VV) phase in RE, should anticipate (contextual) violations of non-functional requirements. 

Treating a detected violation at design time may correspond to actions such as making a different choice for underlying components used by this alternative's tasks, optimizing its behaviour specification or even the disposal of this alternative as a means to satisfy its goal if there is at least one other valid alternative. PMC technique also allows the identification of system alternatives with more influence on each metric through sensitive analysis. 

By coupling a formal verification to a goal model, our approach benefit from a clearer understanding of the system-to-be and its criticality, justifying the verification of system parts or the system whole. The automatic generation of the verification model from an extended version of the goal mode, namely the runtime goal model, greatly reduces the verification overhead. Finally, including context effects provide a more realistic representation of the system to be verified.

Runtime self-adaptation is beyond the scope of this work. However, based on the contextual analysis provided by the CGM and the enriched non-functional and dependability analysis provided by the verification of different alternatives using the PMC technique, it should not be difficult to extend the approach with the additional monitoring, planing and execution capabilities of a self-adaptation loop and have a self-adaptive architecture and mechanism reflected upon its runtime goal model requirements. These concerns should be addressed in future work.

%This space variability problem becomes more complex with the contextualization of goals, means and metrics. According to Bosh et al.[cite Bosh 2004], a high degree of variability allow the use of software in a broader range of contexts. Traditional GORE approaches are mostly used to the selection of which single alternative would exist in the system-to-be for some static context. A dynamic environment can result in contextual violations that can not be resolved just by configuration management. In these cases, a space variability may be required to keep the system properly running in different contexts.


%to a specific kind of goal called \textit{softgoal}. Softgoals are goals for which there is no clear-cut criteria. Often, they represent qualitative intentions of stakeholders, in contrast to the 

\section{Evaluation}

This proposal was evaluated with the application of the extended TROPOS methodology to the development of a Mobile Personal Emergency Response System (MPERS). This system may be seen as a body area network (BAN) with extended functionalities related to ubiquitous emergency response running in a mobile device [BAD]. Instead of a home or hospital static environment, the MPERS is conceived to allow patients with different health risk degrees to maintain mobility while they are monitored and assisted. If a medical emergency is detected, a geolocation feature should point out the location where the emergency response team must be addressed to. 

The MPERS features were based on real emergency response systems available at the industry and also at the BAN explored in previews work [Fernandes]. 

The evaluation process was focused in revealing the major benefits and limitations of the extended TROPOS proposal. Time to market is an important aspect for any software development methodology. Also, the soundness and precision of the proposed probabilistic verification is crucial and must be evaluated as they should not result in mislead decisions about which alternatives should be used by the system. Instead, they must anticipate any violation that could lead to a system failure, specially severe or catastrophic failures, giving analysts valuable information about where the system requirements and specification should be tailored and improved.

\section{Contributions Summary}

This section summarizes the contributions of this proposal.

%A seguir um resumo das con

\begin{enumerate}

\item A new contribution analysis approach for the TROPOS Goal-oriented software development methodology.

\item Inclusion of context effects over goals, means and metrics in the probabilistic model using appropriate constructs and parameters for each case.

\item Conversion rules between different decomposition and runtime constraints in a runtime goal model to a probabilistic model in PRISM language.

\item A parser implementation for the regular expression (regex) language used in runtime goal models with support for execution order, cardinality, alternative execution, optional execution and conditional execution. 

\item An automatic generation of the PRISM model representing activities from a runtime goal model annotated with the runtime regex and graphically modelled using the TAOM4E tool that supports TROPOS methodology.

\end{enumerate}

\section{Document Organization}

This dissertation is organized as follows. Chapter~\ref{ch_baseline} presents the base concepts of this work and the most important related works. Chapter~\ref{ch_problem} details the problem tackled by this proposal. Chapter~\ref{ch_proposal} presents the new extended TROPOS methodology, the rules for the translation between the contextual goal model and the probabilistic verification model, the parser for the runtime regex and finally the implementation approach for the automatic generation of the probabilistic model in PRISM language. Chapter~\ref{ch_evaluation} evaluates the proposal and describes its benefits and limitations. Finally, Chapter~\ref{ch_conclusion} concludes this work with final considerations about the current proposal, related proposals and our future work.